------------------------------------------------------------------------------------------------------------------------------------
# Start all the Hadoop Services
 - cd /usr/local/hadoop/sbin 
 - start-all.sh
------------------------------------------------------------------------------------------------------------------------------------
# Load the data.csv file into HDFS 
 - hadoop fs -put  ~/project-folder/data/data.csv  hdfs://localhost:54310/data
------------------------------------------------------------------------------------------------------------------------------------
# Show the content of the file 
 - hadoop fs -cat hdfs://localhost:54310/data/
------------------------------------------------------------------------------------------------------------------------------------
# Mongo-Spark-Connector 
 - cd /usr/local/spark/jars
 - wget https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.3.5/mongo-spark-connector_2.11-2.3.5.jar

 # Mongo Java Driver
 - cd /usr/local/spark/jars
 - wget https://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/3.12.11/mongo-java-driver-3.12.11.jar

 # Map-Reduce Job
 - hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.10.0.jar -file mapper.py -mapper mapper.py -file reducer.py -reducer reducer.py -input hdfs://localhost:54310/data -output output12/
 